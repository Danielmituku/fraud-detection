{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Building and Training\n",
        "\n",
        "This notebook covers:\n",
        "- Data preparation and stratified splitting\n",
        "- Baseline model (Logistic Regression)\n",
        "- Ensemble models (Random Forest, XGBoost, LightGBM)\n",
        "- Cross-validation\n",
        "- Model comparison and selection\n",
        "\n",
        "**Author**: Adey Innovations Inc. Data Science Team  \n",
        "**Date**: December 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                            precision_recall_curve, average_precision_score,\n",
        "                            f1_score, precision_score, recall_score, roc_auc_score)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional: XGBoost and LightGBM\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available\")\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "    print(\"LightGBM not available\")\n",
        "\n",
        "# Add parent directory\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from src.modeling import *\n",
        "from src.visualization import plot_confusion_matrix, plot_precision_recall_curve, plot_roc_curve\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare fraud data (run feature engineering first or load processed data)\n",
        "from src.data_loader import load_fraud_data, load_ip_to_country, map_ip_to_country\n",
        "from src.feature_engineering import (create_time_features, create_transaction_velocity_features,\n",
        "                                      create_device_features, encode_categorical_features,\n",
        "                                      prepare_features_for_modeling)\n",
        "\n",
        "# Load data\n",
        "fraud_df = load_fraud_data('../data/raw/Fraud_Data.csv')\n",
        "ip_country_df = load_ip_to_country('../data/raw/IpAddress_to_Country.csv')\n",
        "\n",
        "# Apply feature engineering pipeline\n",
        "fraud_df = map_ip_to_country(fraud_df, ip_country_df)\n",
        "fraud_df = create_time_features(fraud_df)\n",
        "fraud_df = create_transaction_velocity_features(fraud_df)\n",
        "fraud_df = create_device_features(fraud_df)\n",
        "fraud_df, _ = encode_categorical_features(fraud_df, ['source', 'browser', 'sex', 'country'])\n",
        "\n",
        "# Prepare features\n",
        "X, y = prepare_features_for_modeling(fraud_df, target_col='class')\n",
        "print(f\"Features: {X.shape}, Target: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stratified train-test split\n",
        "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Data Split:\")\n",
        "print(f\"Training: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining class distribution:\")\n",
        "print(y_train.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply SMOTE to training data\n",
        "X_train_smote, y_train_smote = apply_smote(X_train, y_train)\n",
        "\n",
        "print(\"After SMOTE:\")\n",
        "print(f\"Training: {X_train_smote.shape[0]} samples\")\n",
        "print(pd.Series(y_train_smote).value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline Model - Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Logistic Regression\n",
        "lr_model = train_logistic_regression(X_train_smote, y_train_smote)\n",
        "\n",
        "# Evaluate\n",
        "lr_metrics = evaluate_model(lr_model, X_test, y_test)\n",
        "\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Precision: {lr_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {lr_metrics['recall']:.4f}\")\n",
        "print(f\"F1-Score: {lr_metrics['f1_score']:.4f}\")\n",
        "print(f\"ROC-AUC: {lr_metrics['roc_auc']:.4f}\")\n",
        "print(f\"AUC-PR: {lr_metrics['average_precision']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(lr_metrics['classification_report'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Logistic Regression results\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "y_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_lr)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Confusion Matrix - Logistic Regression')\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba_lr)\n",
        "axes[1].plot(recall, precision, color='blue', linewidth=2)\n",
        "axes[1].fill_between(recall, precision, alpha=0.3)\n",
        "axes[1].set_xlabel('Recall')\n",
        "axes[1].set_ylabel('Precision')\n",
        "axes[1].set_title(f'PR Curve (AUC={lr_metrics[\"average_precision\"]:.3f})')\n",
        "\n",
        "# Feature Importance (coefficients)\n",
        "importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': np.abs(lr_model.coef_[0])\n",
        "}).sort_values('importance', ascending=False).head(10)\n",
        "axes[2].barh(importance['feature'], importance['importance'])\n",
        "axes[2].invert_yaxis()\n",
        "axes[2].set_xlabel('Absolute Coefficient')\n",
        "axes[2].set_title('Top 10 Feature Importances')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ensemble Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest\n",
        "rf_model = train_random_forest(X_train_smote, y_train_smote, n_estimators=100, max_depth=10)\n",
        "rf_metrics = evaluate_model(rf_model, X_test, y_test)\n",
        "\n",
        "print(\"Random Forest Results:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Precision: {rf_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {rf_metrics['recall']:.4f}\")\n",
        "print(f\"F1-Score: {rf_metrics['f1_score']:.4f}\")\n",
        "print(f\"ROC-AUC: {rf_metrics['roc_auc']:.4f}\")\n",
        "print(f\"AUC-PR: {rf_metrics['average_precision']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost (if available)\n",
        "if XGBOOST_AVAILABLE:\n",
        "    xgb_model = train_xgboost(X_train_smote, y_train_smote, n_estimators=100, max_depth=6)\n",
        "    xgb_metrics = evaluate_model(xgb_model, X_test, y_test)\n",
        "    \n",
        "    print(\"XGBoost Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Precision: {xgb_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {xgb_metrics['recall']:.4f}\")\n",
        "    print(f\"F1-Score: {xgb_metrics['f1_score']:.4f}\")\n",
        "    print(f\"ROC-AUC: {xgb_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"AUC-PR: {xgb_metrics['average_precision']:.4f}\")\n",
        "else:\n",
        "    print(\"XGBoost not available - skipping\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train LightGBM (if available)\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    lgb_model = train_lightgbm(X_train_smote, y_train_smote, n_estimators=100, max_depth=6)\n",
        "    lgb_metrics = evaluate_model(lgb_model, X_test, y_test)\n",
        "    \n",
        "    print(\"LightGBM Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Precision: {lgb_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {lgb_metrics['recall']:.4f}\")\n",
        "    print(f\"F1-Score: {lgb_metrics['f1_score']:.4f}\")\n",
        "    print(f\"ROC-AUC: {lgb_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"AUC-PR: {lgb_metrics['average_precision']:.4f}\")\n",
        "else:\n",
        "    print(\"LightGBM not available - skipping\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "models = {'Logistic Regression': lr_model, 'Random Forest': rf_model}\n",
        "if XGBOOST_AVAILABLE:\n",
        "    models['XGBoost'] = xgb_model\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    models['LightGBM'] = lgb_model\n",
        "\n",
        "comparison_df = compare_models(models, X_test, y_test)\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "metrics = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'AUC-PR']\n",
        "x = np.arange(len(comparison_df))\n",
        "width = 0.15\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax.bar(x + i*width, comparison_df[metric], width, label=metric)\n",
        "\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x + width * 2)\n",
        "ax.set_xticklabels(comparison_df['Model'])\n",
        "ax.legend(loc='lower right')\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Selection and Justification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model based on F1-Score (balances precision and recall)\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL SELECTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "SELECTED MODEL: {best_model_name}\n",
        "\n",
        "JUSTIFICATION:\n",
        "1. Performance: Highest F1-Score balances precision (avoiding false positives \n",
        "   that frustrate customers) with recall (catching actual fraud).\n",
        "   \n",
        "2. For fraud detection, we prioritize:\n",
        "   - High Recall: Catching as much fraud as possible\n",
        "   - Reasonable Precision: Minimizing false alarms\n",
        "   - AUC-PR: Important for imbalanced datasets\n",
        "   \n",
        "3. The {best_model_name} achieves the best balance of these metrics.\n",
        "\n",
        "4. Additional considerations:\n",
        "   - Interpretability: {'Higher' if 'Logistic' in best_model_name else 'Lower, but SHAP can help'}\n",
        "   - Training time: Fast enough for production\n",
        "   - Inference speed: Suitable for real-time scoring\n",
        "\"\"\")\n",
        "\n",
        "# Save the best model\n",
        "save_model(best_model, f'../models/best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
        "print(f\"\\nModel saved to ../models/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Building and Training\n",
        "\n",
        "This notebook covers:\n",
        "- Data preparation and stratified splitting\n",
        "- Baseline model (Logistic Regression)\n",
        "- Ensemble models (Random Forest, XGBoost, LightGBM)\n",
        "- Cross-validation\n",
        "- Model comparison and selection\n",
        "\n",
        "**Author**: Adey Innovations Inc. Data Science Team  \n",
        "**Date**: December 2025\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
